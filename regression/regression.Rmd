---
title: "Regression - Estimation, Testing and Diagnosis"
author: "SongWang"
date: "04/27/2016"
output: html_document
---


The following data (*bloodpress.txt*) on 20 individuals with high blood pressure
\begin{enumerate}
\item blood pressure (y = BP, in mm Hg)
\item age (x1 = Age, in years)
\item weight (x2 = Weight, in kg)
\item body surface area (x3 = BSA, in sq m)
\item duration of hypertension (x4 = Dur, in years)
\item basal pulse (x5 = Pulse, in beats per minute)
\item stress index (x6 = Stress)
\end{enumerate}

The researchers were interested in determining if a relationship exists between blood pressure and age, weight, body surface area, duration, pulse rate and/or stress level.

```{r}
rm(list=ls())
bloodpress <- read.table("./bloodpress.txt", header = T)
( cor.bp <- round ( cor(bloodpress[,-1]) ,3))
cor.bp *(cor.bp>0.8)
```


###Topic 1 Effects of multicollinearity: 
regress BP on Weight and BSA, check out the changes when a new variable which is correlated to the existing variable is added to the model.

```{r, echo=FALSE}
lm1 <- lm(BP~ Weight, bloodpress)
lm2 <- lm(BP~ BSA, bloodpress)
lm3 <-lm (BP~ Weight+BSA, bloodpress)
lm4 <- lm(BP~ BSA + Weight, bloodpress)

summary(lm1) 
summary(lm2)
summary(lm3)
summary(lm4)
## coefficients changed and standard error of regression coefficients increase when a new variable is added to the model



anova(lm1)
anova(lm2)
anova(lm3)
anova(lm4)
## SS for each variable dependent on what other variables are in the model. the contribution/ importance of one variable is not absolute anymore.
```




###Topic 2, how to detect the multicollinearity -- Variation Inflation Factor.
![VIF definition](vif.png)

```{r}
model1 <- lm(BP~., data = bloodpress[,-1])
summary(model1)
#install.packages("lme4")
#install.packages("car")
library(lme4)
library(car)
car::vif(model1)

```

###Topic 3, regress on categorial and continuous variables -- ANCOVA (analysis of covariance)
```{r}
crates <- data.frame(list(
    Day = 1:9,
    x1 = c(1,1,1,0,0,1,0,1,1),
    x2 = c(0,1,0,1,1,1,1,0,1),
    y  = c(48,51,39,24,24,27,12,27,24)
))

#a) 
Model.1 <- lm(y~ x1+x2 -1, crates)
summary(Model.1)
res <- crates$y - Model.1$fitted.values
sum(crates$x1*res)
sum(crates$x2*res)

## test sum of slope =30?
X <- model.matrix((Model.1))
var.12 <- sum(solve(t(X)%*%X)) * sum(res^2)/(9-2)
t = (sum(Model.1$coefficients)-30)/sqrt(var.12)
(1- pt(q = t, df = 7))*2


## a new factor variable  U
crates$U <- as.factor(c(1,3,1,2,2,3,2,1,3))
Model.2 <- lm(y~U, crates)
anova(Model.2)

#(f)
newdata <- crates
newdata $y.hat <- mean(crates$y)
newdata$'y.bar- y.hat' <- Model.2$fitted.values -newdata$y.hat
newdata$'y - y.bar' <- newdata$y - Model.2$fitted.values
newdata

#(g)
Model.3 <- lm(y~ x1 + x2+ x1*x2 +Day -1 ,crates)
summary(Model.3)
anova(Model.3)
par(mfcol=c(2,2))
plot(Model.3, which = 1)
plot(Model.3, which = 2)
plot(1:9, Model.3$residuals,type = "b")

#(h)
par(mfcol=c(2,2))
plot(Model.1, which =1)
plot(Model.1, which =2)
plot(1:9, Model.1$residuals,type = "b")

par(mfcol=c(2,2))
plot(Model.2, which =1)
plot(Model.2, which =2)
plot(1:9, Model.2$residuals,type = "b")

## time trend is very clear


## catergorical and continuous variables
Model.4 <- lm(y ~ U+Day+Day*U, data = crates)
summary(Model.4)



# verifying the Least square estimate
X <- model.matrix(Model.1)
solve(t(X)%*%X) %*% t(X)%*% crates$y


```

